# ch5. High Performance Computing / 高效能運算

本章將介紹在 openSUSE 16 環境下建置高效能運算 (HPC) 叢集所需的關鍵元件與設定。從基礎的網路架構開始，逐步介紹認證服務、資源排程系統、遠端管理工具，以及環境模組系統。

## 5-0. 網路架構

在一個典型的高效能運算叢集中，通常會有一台或多台主節點 (Server/Controller) 和數台計算節點 (Compute Node)。它們透過網路互相連接，協同工作。

- **伺服器 IP (範例)**: `192.168.122.57` (serv)
- **客戶端 IP (範例)**: `192.168.122.58` (node1)

以下是一個簡易的服務架構圖：

```
            server          node1           node....
            +---------------+---------------+-----------
service     nis server      nis client      nis client
            nfs server      nfs client      nfs client
            ntp client      ntp client      nts client
            munge           munge           munge

slurm       controller      compute         compute
            dbd
            login
```

- **說明**:
  - **基礎服務**:
    - `nis` / `nfs`: 提供統一的帳號認證與家目錄共享，讓使用者可以在任何節點登入並存取相同檔案。
    - `ntp`: 確保叢集中所有節點的時間同步，這對於日誌記錄與作業排程至關重要。
    - `munge`: 提供一個安全的認證機制，用於驗證叢集中不同服務 (如 Slurm) 之間的通訊，確保指令來自可信的來源。
  - **Slurm 排程系統**:
    - `controller`: 叢集的管理者，負責接收使用者的工作請求 (Job)，並將它們分配到計算節點上。
    - `dbd` (Database Daemon): (可選) 將帳號與資源使用資訊記錄到資料庫，用於後續的分析與管理。
    - `compute`: 實際執行計算任務的節點。
    - `login`: (可選) 使用者登入並提交工作的節點，通常與 `controller` 分開以降低負載。

---

## 5-1. MUNGE

MUNGE (MUNGE Uid 'N' Gid Emporium) 是一個認證服務，主要用於在叢集環境中建立信任關係。它透過在服務之間傳遞加密簽章的憑證 (credential)，來驗證使用者和群組的身份。在 Slurm 叢集中，`slurmctld` (主控制器) 和 `slurmd` (計算節點守護程式) 之間的通訊都需要通過 MUNGE 的驗證，以防止未經授權的指令執行。

### 安裝與設定

- **安裝套件**:
  在所有節點 (包含 server 和 compute nodes) 上安裝 `munge`。

  - **`zypper in munge`**: 安裝 MUNGE 套件。
    ```bash
    tux@suse16:~> sudo zypper in munge
    ```

- **產生金鑰**:
  需在 **主節點 (server)** 上產生一把金鑰 (`munge.key`)，然後將這把金鑰安全地複製到所有計算節點。叢集中所有節點的 `munge.key` 必須完全相同，才能互相認證。

  - **`mv`**: 備份預設產生的金鑰。
  - **`mungekey -c -k /etc/munge/munge.key`**: 產生一把新的金鑰。`-c` 代表建立 (create)，`-k` 指定金鑰路徑。
  - **`chown munge:munge /etc/munge/munge.key`**: 將金鑰的擁有者設定為 `munge` 使用者與群組，這是出於安全考量，確保只有 MUNGE 服務能讀取它。
    ```bash
    # 在主節點 (server) 上執行
    tux@suse16:~> sudo mv /etc/munge/munge.key /etc/munge/munge.key.backup
    tux@suse16:~> sudo mungekey -c -k /etc/munge/munge.key
    tux@suse16:~> sudo chown munge:munge /etc/munge/munge.key
    ```
    之後，需要將 `/etc/munge/munge.key` 檔案從主節點複製到所有計算節點的相同路徑下，並確保權限設定也相同。

- **設定與啟動**:
  在所有節點上啟動並設定開機自動執行 `munge` 服務。

  - **`systemctl status munge.service`**: 檢查服務狀態。
  - **`systemctl enable munge.service`**: 設定開機時自動啟動。
  - **`systemctl start munge.service`**: 立即啟動服務。
    ```bash
    tux@suse16:~> sudo systemctl enable munge.service
    tux@suse16:~> sudo systemctl start munge.service
    ```

4.  **測試連線**:
    從一個節點測試與另一個節點 (或本機) 的 MUNGE 通訊是否正常。

    - **`munge -n | ssh <remote_ip> unmunge`**:

      - `munge -n`: 在本機產生一個憑證。
      - `|`: 透過管道 (pipe) 將憑證傳遞給 `ssh`。
      - `ssh <remote_ip> unmunge`: 登入遠端主機，並執行 `unmunge` 來解碼收到的憑證。

      ```bash
      # 從 server 測試與 localhost 的通訊
      tux@suse16:~> munge -n | ssh tux@localhost unmunge
      (tux@localhost) Password:
      ```

    - **成功訊息**:
      如果金鑰相同且服務正常，會顯示類似以下的成功狀態。

      ```
      STATUS:          Success (0)
      ENCODE_HOST:     ??? (0.0.0.0)
      ENCODE_TIME:     2025-10-14 10:14:19 +0800 (1760408059)
      DECODE_TIME:     2025-10-14 10:14:23 +0800 (1760408063)
      TTL:             300
      CIPHER:          aes128 (4)
      MAC:             sha256 (5)
      ZIP:             none (0)
      UID:             root (0)
      GID:             root (0)
      LENGTH:          0
      ```

    - **常見錯誤**:
      - `unmunge: Error: Invalid credential`: 表示兩台主機的 `munge.key` 不匹配。
      - `unmunge: Error: Failed to access "/run/munge/munge.socket.2": No such file or directory`: 表示遠端主機上的 `munged` 服務沒有啟動。

### 練習

1.  **金鑰同步**: 在主節點上產生一把新的 `munge.key`。使用 `scp` 指令將其安全地複製到 `node1` 計算節點。
2.  **權限檢查**: 確保 `node1` 上的 `munge.key` 檔案擁有者與權限和主節點上的一致 (`munge:munge`, `400`)。
3.  **雙向測試**: 從主節點測試與 `node1` 的 MUNGE 通訊，然後再從 `node1` 測試與主節點的通訊，確保雙方都能成功驗證。

---

## 5-2. 排程系統 (Queuing System)

排程系統是 HPC 叢集的核心，負責管理與分配計算資源 (如 CPU、記憶體、GPU)。它允許使用者提交計算任務 (Job)，並根據設定的策略 (如優先級、資源需求) 將這些任務排入佇列 (Queue)，等待資源可用時執行。

### 演進歷史

- **早期系統 (DQS, PBS)**: 奠定了批次處理 (Batch System) 的基礎，使用者提交腳本，由系統在適當時機執行。
- **網格運算時代 (SGE, CODINE/GRD)**: 隨著網路發展，排程系統開始支援跨越多個地理位置的叢集，形成了「網格 (Grid)」。Sun Grid Engine (SGE) 是這個時期的代表。
- **開源與大規模化 (TORQUE, Slurm)**:
  - **TORQUE**: OpenPBS 的一個分支，功能強大且穩定，常與 Maui/Moab 排程器搭配使用。
  - **Slurm (Simple Linux Utility for Resource Management)**: 現今最主流的開源排程系統之一。它設計簡潔、擴充性極強，能管理從小型叢集到世界頂級超級電腦的龐大資源。因其高效能與靈活性，已成為許多 HPC 環境的首選。
- **商業方案 (LSF)**: IBM 的 LSF 是功能強大的商業排程系統，取代了早期的 LoadLeveler。

本章將專注於 **Slurm** 的安裝與設定。

### 5-2-1. Slurm Controller (主控制器)

Slurm Controller (`slurmctld`) 是叢集的大腦，負責監控所有節點與工作、管理工作佇列，並決定哪個工作在哪個節點上執行。它需要與所有計算節點上的 `slurmd` 守護程式通訊。

- **前置需求**: [MUNGE](#5-1-munge), [NIS Server](../content/ch04.md#4-4-2-nis-server), [NFS Server](../content/ch04.md#4-5-1-nfs-server-設定), [NTP](../content/ch04.md#4-2-1-chrony) 時間同步。

- **安裝套件**:

  - **`slurm`**: 包含 `slurmctld` 以及 `sinfo`, `sbatch` 等管理工具。
  - **`mailx`**: (可選) Slurm 可透過 `mailx` 在工作完成或失敗時寄送通知信。
    ```bash
    server:~ > sudo zypper in slurm mailx
    ```

- **建立設定檔**:
  Slurm 的設定相當複雜，但可透過官方的 [線上設定工具](https://slurm.schedmd.com/configurator.html) 快速產生一個基礎範本。

  - **`cp ...`**: 備份預設設定檔。
  - **`vi ...`**: 修改設定檔為符合自己環境的內容。
  - **`mkdir ...`**: 建立日誌目錄。
  - **`chown ...`**: 確保 `slurm` 使用者有權限寫入日誌目錄。

    ```bash
    server:~ > sudo cp /etc/slurm/slurm.conf /etc/slurm/slurm.conf.backup
    server:~ > sudo vi /etc/slurm/slurm.conf

    server:~ > sudo mkdir -p /var/log/slurm/
    server:~ > sudo chown slurm:slurm /var/log/slurm/
    ```

- **啟動服務與防火牆**:

  - **`6817`**: `slurmctld` (主控制器) 使用。
  - **`6818`**: `slurmd` (計算節點) 使用。
  - **`6819`**: `slurmdbd` (資料庫服務) 使用。

    ```bash
    server:~ > sudo systemctl status slurmctld.service
    server:~ > sudo systemctl enable slurmctld.service
    server:~ > sudo systemctl start slurmctld.service
    server:~ > sudo firewall-cmd --permanent --add-port=6817-6819/tcp
    server:~ > sudo firewall-cmd --reload
    ```

#### `slurm.conf` 範例與解釋

```conf
# /etc/slurm/slurm.conf
# --- 叢集基本設定 ---
ClusterName=hpc                                # 叢集名稱
SlurmctldHost=server                           # 主控制器的主機名稱
SlurmUser=slurm                                # 執行 Slurm 服務的使用者
SlurmctldPidFile=/var/run/slurm/slurmctld.pid  # 主控制器 PID 檔案路徑
SlurmctldPort=6817                             # 主控制器監聽的 port
SlurmctldLogFile=/var/log/slurm/slurmctld.log  # 主控制器日誌檔案

# --- 計算節點設定 ---
SlurmdPidFile=/var/run/slurm/slurmd.pid        # 計算節點 PID 檔案路徑
SlurmdPort=6818                                # 計算節點監聽的 port
SlurmdSpoolDir=/var/spool/slurm                # 計算節點的工作暫存目錄
SlurmdLogFile=/var/log/slurm/slurmd.log        # 計算節點日誌檔案

# --- 資源與排程 ---
ProctrackType=proctrack/cgroup                 # 使用 cgroup 來追蹤與管理工作程序
SchedulerType=sched/backfill                   # 排程策略，backfill 能將小工作安插到大工作的空檔，提高資源利用率
SelectType=select/cons_tres                    # 資源選擇插件，cons_tres (Consumable Trackable Resources) 是推薦的選擇

# --- 帳號與紀錄 (初期可先關閉) ---
AccountingStorageType=accounting_storage/none  # 暫不使用帳號紀錄資料庫
JobCompType=jobcomp/none                       # 暫不紀錄工作完成資訊

# --- 節點與分割區定義 ---
# 定義名為 node1, node2 的計算節點
NodeName=node[1-2] State=Idle
# 建立一個名為 normal 的分割區 (Partition)，包含 node1, node2
# Default=YES: 預設分割區, MaxTime: 工作最長執行時間, State=UP: 分割區啟用
PartitionName=normal Nodes=node[1-2] Default=YES MaxTime=24:00:00 State=UP
```

### 5-2-2. Slurm Compute (計算節點)

計算節點 (`slurmd`) 負責接收來自 `slurmctld` 的指令，並在本地執行計算任務、回報節點狀態。

**前置需求**: [MUNGE](#5-1-munge) (金鑰需與 server 同步), [NIS Client](../content/ch04.md#4-4-3-nis-client), [NFS Client](../content/ch04.md#4-5-2-nfs-client), [NTP](../content/ch04.md#4-2-1-chrony) 時間同步。

- **同步設定**:

  - **`munge.key`**: 計算節點的 `munge.key` 必須與主控制器完全一致。
  - **`slurm.conf`**: 計算節點的 `slurm.conf` 也必須與主控制器完全一致，這樣它才知道叢集的結構以及主控制器在哪裡。
    ```bash
    # 在 node 上執行
    node:~ > sudo scp <server_ip>:/etc/munge/munge.key /etc/munge/
    node:~ > sudo scp <server_ip>:/etc/slurm/slurm.conf /etc/slurm/
    ```

- **安裝套件**:

  - **`slurm-node`**: 包含 `slurmd` 守護程式。
    ```bash
    node:~ > sudo zypper in slurm-node
    ```

- **檢查設定與建立目錄**:

  - **`slurmd -C`**: 檢查硬體資源並顯示 Slurm 將如何識別此節點。驗證設定檔是否正確讀取節點資訊。
  - **`mkdir` / `chown`**: 建立日誌目錄並設定權限。
    ```bash
    node:~ > sudo slurmd -C
    node:~ > sudo mkdir -p /var/log/slurm/
    node:~ > sudo chown slurm:slurm /var/log/slurm/
    ```

- **啟動服務與防火牆**:

  ```bash
  node:~ > sudo systemctl enable slurmd.service
  node:~ > sudo systemctl start slurmd.service

  node:~ > sudo firewall-cmd --permanent --add-port=6817-6819/tcp
  node:~ > sudo firewall-cmd --reload
  ```

### Slurm 基本指令

在主節點或登入節點上執行。

- **`sinfo`**: 顯示分割區 (Partition) 與節點 (Node) 的狀態。

  ```bash
  server:~ > sinfo
  PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
  normal*      up 1-00:00:00      2   idle node[1-2]
  ```

  - `STATE`: `idle` (空閒), `alloc` (已分配), `mix` (部分分配), `down` (故障)。

- **`sbatch`**: 提交一個批次處理工作。

  ```bash
  # 準備一個工作腳本
  server:~ > vi job.slurm
  # 提交工作
  server:~ > sbatch job.slurm
  Submitted batch job 1
  ```

- **`squeue`**: 查看正在排隊 (`PD`) 或正在執行 (`R`) 的工作。

  ```bash
  server:~> squeue
               JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
                   1    normal     test      tux  R       0:02      1 node1
  ```

- **`scancel`**: 取消一個工作。

  ```bash
  server:~> scancel 1
  ```

- **`scontrol`**: 功能強大的管理工具，可用於查看或修改節點、工作、分割區的詳細資訊。
  ```bash
  server:~> scontrol show node node1   # 顯示 node1 的詳細狀態
  server:~> scontrol show job 1        # 顯示 job id 為 1 的詳細資訊
  ```

#### 工作腳本範例 (`job.slurm`)

```bash
#!/bin/sh
#SBATCH -J test                # 工作名稱 (Job Name)
#SBATCH -o %j.out              # 標準輸出重新導向到 <job_id>.out
#SBATCH -e %j.err              # 標準錯誤重新導向到 <job_id>.err
#SBATCH --nodes=1              # 需要 1 個節點
#SBATCH --ntasks-per-node=1    # 每個節點執行 1 個任務

# --- 以下為實際要執行的指令 ---
echo "工作開始於: $(date)"
hostname                       # 顯示執行此工作的主機名稱
sleep 60                       # 模擬一個執行 60 秒的計算
echo "工作結束於: $(date)"
```

### 練習

1.  **新增節點**: 有一台新的計算節點 `node3`，請完成將它加入 Slurm 叢集的所有步驟 (MUNGE, `slurm.conf` 修改與同步, `slurmd` 啟動)。
2.  **提交工作**: 撰寫一個 `sbatch` 腳本，使其在叢集中的任何一個節點上執行 `uptime` 和 `whoami` 指令，並將結果輸出到檔案。
3.  **資源監控**: 使用 `sinfo` 和 `squeue` 指令，在工作執行期間監控節點狀態與工作佇列。工作結束後再次確認。

---

## 5-3. Remote Shell

在管理一個叢集時，常常需要在多台機器上執行相同的指令。使用 `ssh` 逐一登入非常沒有效率，因此需要 `pdsh` 這類的平行分散式 Shell 工具。

### 5-3-1. PDSH (Parallel Distributed Shell)

`pdsh` 允許使用者在多個遠端主機上並行執行指令。

- **安裝套件**:
  在主節點或管理節點上安裝。

  ```bash
  server:~ > sudo zypper in pdsh
  ```

- **設定無密碼登入**:
  `pdsh` 依賴 `ssh` 進行連線。為了避免每次執行都要輸入密碼，需要設定 SSH key-based 的無密碼登入。

  - **`ssh-keygen`**: 在主節點上為目前使用者產生一對 SSH 公私鑰。
  - **`ssh-copy-id <node>`**: 將公鑰複製到目標節點的 `~/.ssh/authorized_keys` 檔案中。
    ```bash
    # 在主節點上執行
    server:~ > ssh-keygen
    server:~ > ssh-copy-id node1
    server:~ > ssh-copy-id node2
    ```

- **執行指令**:

  - **`-w node1,node2,...`**: 指定要執行指令的目標主機列表。
  - **`-w node[1-2]`**: 也可使用主機名稱表達式。

    ```bash
    # 在 node1 和 node2 上執行 hostname 指令
    server:~ > pdsh -w node1,node2 hostname
    node1: node1
    node2: node2

    # 一次關閉所有計算節點
    server:~ > sudo pdsh -w node[1-2] poweroff
    ```

### 練習

1.  **設定 SSH 連線**: 確保從主節點無密碼 `ssh` 登入所有計算節點。
2.  **平行檢查**: 使用 `pdsh` 在所有計算節點上同時執行 `systemctl status slurmd`，快速檢查所有節點的服務狀態。
3.  **檔案分發**: 雖然 `scp` 能用，但嘗試結合 `pdsh` 與 `tar` 或 `rsync`，思考如何將一個目錄從主節點高效地分發到所有計算節點。

---

## 5-4. 環境模組系統 (Environment Module System)

在 HPC 環境中，常常需要使用不同版本或由不同編譯器建構的軟體 (如 `gcc/9.1`, `gcc/10.2`, `intel-mpi/2021`)。如果將這些軟體的路徑全部寫入 `.bashrc`，會造成環境變數 (`PATH`, `LD_LIBRARY_PATH`) 的混亂與衝突。

環境模組系統就是為了解決這個問題而生。允許使用者動態修改自己的 Shell 環境，輕鬆地載入、卸載或切換不同的軟體版本。

### 演進歷史

- **Environment Modules (Tcl-based)**: 最早且最廣泛使用的模組系統，使用 Tcl 語言撰寫模組檔案 (modulefile)。
- **Lmod (Lua-based)**: 由 TACC (Texas Advanced Computing Center) 開發的現代化模組系統。它使用 Lua 語言，並引入了階層式 (hierarchical) 模組管理，能更好地處理模組之間的依賴關係 (例如，載入 `intel-mpi` 前必須先載入 `intel-compiler`)。

### 5-4-1. Lmod

Lmod 是目前許多現代 HPC 叢集的首選。

- **安裝**:

  ```bash
  server:~ > sudo zypper in lua-lmod
  ```

  安裝後，通常需要重新登入或 `source /etc/profile.d/lmod.sh` 來讓 `module` 指令生效。

- **基本指令**:
  `ml` 是 `module` 的一個方便的縮寫。

  - **`echo $MODULEPATH`**: 顯示 Lmod 會去搜尋模組檔案的目錄路徑。
  - **`ml avail`**: 列出所有可用的模組。
  - **`ml list`**: 列出目前已載入的模組。
  - **`ml load <module>`**: 載入一個模組 (會修改 `PATH` 等環境變數)。
  - **`ml unload <module>`**: 卸載一個模組 (會將環境變數還原)。
  - **`ml swap <old> <new>`**: 切換模組，例如從 `gcc/9` 切換到 `gcc/10`。
  - **`ml show <module>`**: 顯示某個模組檔案的內容，以及它會如何修改環境。

- **建立自訂模組範例**:
  簡單的 `hello` 程式建立一個 Lmod 模組。

  - **建立程式**:

    ```bash
    # 建立安裝目錄與程式
    server:~ > sudo mkdir -p /opt/hello/bin
    server:~ > sudo tee /opt/hello/bin/hello > /dev/null <<'EOF'
    #!/bin/bash
    echo "Hello from the custom module!"
    EOF
    server:~ > sudo chmod +x /opt/hello/bin/hello
    ```

  - **建立 Modulefile**:
    Lmod 的模組檔案是用 Lua 寫的。我們在系統預設路徑下建立對應的檔案。

    ```bash
    # 建立模組檔案目錄與檔案
    server:~ > sudo mkdir -p /usr/share/lmod/modulefiles/hello
    server:~ > sudo tee /usr/share/lmod/modulefiles/hello/1.0.lua > /dev/null <<'EOF'
    -- Help message for the module
    help([[
      This module provides a simple hello program version 1.0
    ]])

    -- Add the program's bin directory to the PATH
    prepend_path("PATH", "/opt/hello/bin")
    EOF
    ```

  - **測試模組**:

    ```bash
    # 重新掃描模組，或等待快取更新
    server:~ > module avail

    # 載入模組
    server:~ > ml load hello/1.0

    # 檢查已載入模組
    server:~ > ml list

    # 執行程式
    server:~ > hello
    Hello from the custom module!

    # 卸載模組
    server:~ > ml unload hello/1.0
    ```

### 練習

1.  **建立自訂模組目錄**: 在家目錄下建立 `~/privatemodules`，並將其路徑加入到 `MODULEPATH` 環境變數中。
2.  **撰寫簡單模組檔案**: 在 `~/privatemodules` 中為一個假想的軟體 `my-app/1.0` 撰寫一個 Lua 模組檔案。這個模組需要將 `/opt/my-app/1.0/bin` 加入到 `PATH` 環境變數。
    ```lua
    -- ~/privatemodules/my-app/1.0.lua
    help("This is my custom application version 1.0")
    prepend_path("PATH", "/opt/my-app/1.0/bin")
    ```
3.  **測試模組**: 使用 `ml avail` 確認能看到 `my-app/1.0`，然後用 `ml load my-app/1.0` 載入它，並用 `echo $PATH` 檢查路徑是否已成功加入。最後用 `ml unload my-app/1.0` 將其卸載。

---

## 5-5. Compiler / 編譯器

在 HPC 領域，編譯器扮演著至關重要的角色。不僅是將原始碼轉換為可執行檔的工具，更是榨取硬體極致效能的關鍵。選擇不同的編譯器或調整其編譯選項，對程式的執行速度可能產生數倍的影響。

### 發展歷史與主要廠商

- **GNU Compiler Collection (GCC)**: 作為開源世界的標準，GCC (`gcc`, `g++`, `gfortran`) 提供了對多種程式語言和硬體架構的廣泛支援。它功能全面、穩定可靠，是所有 Linux 發行版的預設編譯器。

- **Intel Compilers**: Intel 的編譯器套件 (傳統上稱為 `icc`, `icpc`, `ifort`) 以其對自家 CPU 架構的深度優化而聞名。它能產生高度優化的向量化指令 (SIMD)，在科學與工程計算中往往能帶來顯著的效能提升。現在，這些編譯器已整合到 **Intel oneAPI** 工具包中。

- **Nvidia HPC SDK (前身為 PGI)**: Portland Group, Inc. (PGI) 曾是 HPC 編譯器領域的佼佼者，特別是在 Fortran 和異質運算方面。被 Nvidia 收購後，其技術與 Nvidia 的 CUDA 平台深度整合，演變為 **Nvidia HPC SDK**。此 SDK 包含 `nvc` (C), `nvc++` (C++), `nvfortran` (Fortran) 等編譯器，專為在 CPU (x86, Arm) 和 Nvidia GPU 上進行高效能計算而設計，是 GPU 加速運算的首選。

### 5-5-1. GNU Compiler Collection (GCC)

作為開源世界的標準，GCC (`gcc`, `g++`, `gfortran`) 提供了對多種程式語言和硬體架構的廣泛支援。它功能全面、穩定可靠，是所有 Linux 發行版的預設編譯器，也是編譯許多開源軟體的基礎。

- **安裝與設定**

  在 openSUSE 上，可以透過 `zypper` 安裝 C, C++, 和 Fortran 的編譯器。

  - **`gcc`**: 提供 C++ 編譯器 `gcc`。
  - **`gcc-c++`**: 提供 C++ 編譯器 `g++`。
  - **`gcc-fortran`**: 提供 Fortran 編譯器 `gfortran`。

  ```bash
  server:~ > sudo zypper in -t pattern devel_basis
  server:~ > sudo zypper in gcc gcc-c++ gcc-fortran
  ```

- **編譯範例**
  載入模組後，就可使用 Intel 編譯器。

  - **`gcc`**: GNU project C and C++ compiler。
  - **`g++`**: `gcc` 的 C++ 模式別名。
  - **`gfortran`**: GNU Fortran compiler。

  ```bash
  # 編譯 C
  server:~ > gcc -o hello_c hello.c

  # 編譯 C++
  server:~ > g++ -o hello_cpp hello.cpp

  # 編譯 Fortran 77 / fixed format
  server:~ > gfortran -o hello_f77 -ffixed-form hello.f

  # 編譯 Fortran 90 / free format
  server:~ > gfortran -o hello_f90 -ffree-form hello.f90
  ```

### 5-5-2. Intel® oneAPI HPC Toolkit

整合性的開發工具包，不僅包含 C/C++/Fortran 編譯器，還提供了 MPI 函式庫、數學核心函式庫 (MKL)、效能分析工具 (VTune Profiler) 等，旨在為 Intel 的 CPU 和 GPU 提供統一的開發體驗。透過[官方網站](https://www.intel.com/content/www/us/en/developer/tools/oneapi/hpc-toolkit-download.html)下載。

Intel oneAPI 是一個整合性的開發工具包，提供編譯器、函式庫與分析工具。以下是安裝與設定步驟。

- **安裝與設定**

  - **安裝依賴套件**: Intel oneAPI 的安裝程式需要 `tar` 與 C++ 編譯環境。
    ```bash
    server:~ > sudo zypper in tar gcc-c++
    ```
  - **執行安裝腳本**: 從 [Intel 官網](https://www.intel.com/content/www/us/en/developer/tools/oneapi/hpc-toolkit-download.html) 下載離線安裝包後，執行 shell 腳本進行安裝。
    ```bash
    server:~ > sudo sh intel-oneapi-hpc-toolkit-2025.2.0.575_offline.sh
    ```
  - **設定環境變數**: 安裝後，需要 `source` 官方提供的 `oneapi-vars.sh` 腳本來設定所有必要的環境變數。為了方便管理，我們不建議直接將此行加入 `.bashrc`，而是為其建立一個 Lmod 模組。
    ```bash
    # 手動設定 (僅當前 session 有效)
    server:~ > source /opt/intel/oneapi/2025.2/oneapi-vars.sh
    ```

- **建立 Lmod 模組檔案**
  為了能方便地載入與卸載 Intel oneAPI 環境，為其撰寫一個模組檔案。

  ```bash
  server:~ > sudo mkdir -p /usr/share/lmod/modulefiles/intel/oneapi
  server:~ > sudo vi /usr/share/lmod/modulefiles/intel/oneapi/2025.2.lua
  ```

  將以下 Lua 腳本內容貼入檔案中。這個腳本模擬了 `oneapi-vars.sh` 的行為，設定了 `PATH`, `LD_LIBRARY_PATH` 等重要變數。

  ```lua
  -- -*- lua -*-
  -- Module file for Intel oneAPI Environment version 2025.2
  whatis("Name", "oneAPI Base Environment")
  whatis("Version", "2025.2")
  whatis("Description", "Sets up the core environment for the Intel oneAPI HPC Toolkit.")
  whatis("URL", "https://www.intel.com/content/www/us/en/developer/tools/oneapi/overview.html")

  -- 安裝根目錄
  local base = "/opt/intel/oneapi/2025.2"

  -- 設定環境變數
  setenv("ONEAPI_ROOT", base)
  prepend_path("PATH", pathJoin(base, "bin"))
  prepend_path("MANPATH", pathJoin(base, "share/man"))
  prepend_path("C_INCLUDE_PATH", pathJoin(base, "include"))
  prepend_path("CPLUS_INCLUDE_PATH", pathJoin(base, "include"))
  prepend_path("LIBRARY_PATH", pathJoin(base, "lib"))
  prepend_path("LD_LIBRARY_PATH", pathJoin(base, "lib"))
  prepend_path("PKG_CONFIG_PATH", pathJoin(base, "lib/pkgconfig"))

  -- 輔助說明訊息
  help([[
  This module sets up the environment for the Intel oneAPI HPC Toolkit 2025.2.
  It configures PATH, library paths, and other variables.
  ]])
  ```

- **編譯範例**
  載入模組後，就可使用 Intel 編譯器。

  - **`icx`**: Intel oneAPI DPC++/C++ Compiler。
  - **`icpx`**: `icx` 的 C++ 模式別名。
  - **`ifx`**: Intel Fortran Compiler (LLVM-based)。

  ```bash
  # 載入模組
  server:~ > ml load intel/oneapi/2025.2

  # 編譯 C
  server:~ > icx -o hello_c hello.c

  # 編譯 C++
  server:~ > icpx -o hello_cpp hello.cpp

  # 編譯 Fortran 77 / fixed format
  server:~ > ifx -o hello_f77 -fixed hello.f

  # 編譯 Fortran 90 / free format
  server:~ > ifx -o hello_f90 -free hello.f90
  ```

### 5-5-3. NVIDIA HPC SDK

專為科學與工程領域設計的編譯器、函式庫和工具集合。它支援使用標準 C++, Fortran 和 Python 進行 GPU 加速，並提供 OpenACC 和 OpenMP 等平行編程模型的強大支援。透過[官方網站](https://developer.nvidia.com/hpc-sdk)下載。

- **安裝與設定**

  - **解壓縮並安裝**: 從 [NVIDIA 官網](https://developer.nvidia.com/hpc-sdk)下載後，解壓縮並執行安裝程式。
    ```bash
    server:~ > tar zxf nvhpc_2025_257_Linux_x86_64_cuda_12.9.tar.gz
    server:~ > cd nvhpc_2025_257_Linux_x86_64_cuda_12.9
    server:~/... > sudo ./install
    ```
  - **手動設定環境**: 為了讓 Shell 找到編譯器，需要手動匯出 (export) 環境變數。同樣地，將這些設定包裝成 Lmod 模組是更好的實踐。
    ```bash
    server:~ > export NVARCH=`uname -s`_`uname -m`
    server:~ > export NVCOMPILERS=/opt/nvidia/hpc_sdk
    server:~ > export MANPATH=$MANPATH:$NVCOMPILERS/$NVARCH/25.7/compilers/man
    server:~ > export PATH=$NVCOMPILERS/$NVARCH/25.7/compilers/bin:$PATH
    ```

- **建立 Lmod 模組檔案**

  NVIDIA HPC SDK 有提供 modulfe file，可直接使用

  ```bash
  server:~ > setenv MODULEPATH $NVCOMPILERS/modulefiles:"$MODULEPATH"
  server:~ > module add nvhpc
  ```

- **編譯範例**

  - **`nvc`**: C 編譯器。
  - **`nvc++`**: C++ 編譯器。
  - **`nvfortran`**: Fortran 編譯器。

  ```bash
  # 編譯 C 語言
  server:~ > nvc -o hello_c hello.c

  # 編譯 C++
  server:~ > nvc++ -o hello_cpp hello.cpp

  # 編譯 Fortran 77 / fixed format
  server:~ > nvfortran -o hello_f77 -Mfixed hello.f

  # 編譯 Fortran 90 / free format
  server:~ > nvfortran -o hello_f90 -Mfree hello.f90
  ```

### 練習

1.  **編譯與執行**: 使用 `gcc`, `icx` 和 `nvc` 分別編譯 `hello.c` 範例程式，並執行它們，確認輸出結果。
2.  **編譯與執行**: 使用 `g++`, `icpx` 和 `nvc++` 分別編譯 `hello.cpp` 範例程式，並執行它們，確認輸出結果。
3.  **編譯與執行**: 使用 `gfortran`, `ifx` 和 `nvfortran` 分別編譯 `hello.f77` 範例程式，並執行它們，確認輸出結果。
4.  **編譯與執行**: 使用 `gfortran`, `ifx` 和 `nvfortran` 分別編譯 `hello.f90` 範例程式，並執行它們，確認輸出結果。
5.  **為 GNU 建立模組**: 為 GNU Compiler Collection 撰寫一個 Lmod 模組檔案，將手動設定環境的步驟自動化。
6.  **為 Intel 建立模組**: 為 Intel® oneAPI HPC Toolkit 撰寫一個 Lmod 模組檔案，將手動設定環境的步驟自動化。
7.  **為 NVIDIA 建立模組**: 為 NVIDIA HPC SDK 撰寫一個 Lmod 模組檔案，將手動設定環境的步驟自動化。

---

## 5-6. Chemistry Computer Program / 計算化學應用

HPC 叢集的主要目的是執行大型、複雜的科學與工程計算程式。以下是一些在計算化學、物理與材料科學領域極具代表性的軟體。

- **[ABINIT](https://www.abinit.org/)**: 開源的材料科學模擬軟體，可用於計算光學、介電、聲子等性質。
- **[Gaussian](https://gaussian.com/)**: 功能強大的計算化學軟體，側重於研究分子的結構、能量、光譜與化學性質。
- **[GROMACS (Groningen Machine for Chemical Simulations)](https://www.gromacs.org/)**: 一款速度極快的分子動力學引擎，特別適用於模擬蛋白質、脂質等生物大分子的行為。
- **[LAMMPS (Large-scale Atomic/Molecular Massively Parallel Simulator)](https://www.lammps.org/)**: 一款經典的分子動力學模擬軟體，專長於模擬極大規模的原子與分子系統。
- **[Quantum ESPRESSO](https://www.quantum-espresso.org/)**: 一款開源的電子結構計算與材料模擬軟體，功能與 VASP 類似，在學術界非常受歡迎。
- **[VASP (Vienna Ab initio Simulation Package)](https://www.vasp.at/)**: 使用量子力學進行原子級材料模擬的軟體，廣泛用於計算固態材料、分子與表面的性質。

---

## 5-7. 總結

本章涵蓋了建置基礎 HPC 叢集的幾個核心元件。從清晰的 **網路架構** 規劃開始，逐步建置一個功能性的 HPC 叢集。實作 **MUNGE** 來確保叢集內部通訊的安全。核心部分是 **Slurm 排程系統** 的建置，包括設定主控制器 (`slurmctld`) 和計算節點 (`slurmd`)，並學習了 `sinfo`, `sbatch` 等基本操作指令。為了方便管理，介紹 **PDSH** 工具，以實現在多節點上平行執行指令。探討 **Lmod 環境模組系統**，有效地管理複雜的軟體環境。此外，也介紹了在 HPC 領域至關重要的 **編譯器** (GNU, Intel, Nvidia) 及其發展，並以些計算軟體為例，展示了 HPC 在真實科學研究中的應用。
完成本章後，掌握搭建與管理 Slurm 叢集的基礎技能，也對 HPC 生態系中的關鍵軟體元件有了更深入的了解。
